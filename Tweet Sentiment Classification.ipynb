{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'fear', 'sadness', 'joy']\n",
      " Shape of X is  (3787, 1547)\n",
      " Shape of Y is  (4, 1547)\n",
      " Shape of m is  1547\n",
      " Shape of W1 is  (100, 3787)\n",
      " Shape of W2 is  (4, 100)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.27      0.11       112\n",
      "           1       0.04      0.22      0.07        67\n",
      "           2       0.12      0.26      0.16       174\n",
      "           3       0.80      0.27      0.41      1194\n",
      "\n",
      "   micro avg       0.27      0.27      0.27      1547\n",
      "   macro avg       0.26      0.26      0.19      1547\n",
      "weighted avg       0.64      0.27      0.34      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.36      0.46       715\n",
      "           1       0.22      0.40      0.29       196\n",
      "           2       0.39      0.43      0.41       346\n",
      "           3       0.36      0.50      0.41       290\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      1547\n",
      "   macro avg       0.40      0.42      0.39      1547\n",
      "weighted avg       0.47      0.41      0.42      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.60      0.53       320\n",
      "           1       0.29      0.57      0.39       176\n",
      "           2       0.65      0.48      0.55       521\n",
      "           3       0.70      0.54      0.61       530\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1547\n",
      "   macro avg       0.53      0.55      0.52      1547\n",
      "weighted avg       0.59      0.53      0.55      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.65       633\n",
      "           1       0.43      0.68      0.52       217\n",
      "           2       0.55      0.74      0.63       284\n",
      "           3       0.70      0.68      0.69       413\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1547\n",
      "   macro avg       0.62      0.66      0.62      1547\n",
      "weighted avg       0.69      0.63      0.64      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.70       335\n",
      "           1       0.52      0.76      0.62       241\n",
      "           2       0.77      0.60      0.67       492\n",
      "           3       0.82      0.69      0.75       479\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1547\n",
      "   macro avg       0.69      0.71      0.68      1547\n",
      "weighted avg       0.72      0.69      0.69      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.55      0.70       703\n",
      "           1       0.56      0.77      0.65       253\n",
      "           2       0.51      0.89      0.65       222\n",
      "           3       0.75      0.82      0.78       369\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1547\n",
      "   macro avg       0.69      0.76      0.69      1547\n",
      "weighted avg       0.77      0.70      0.70      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.96      0.59       183\n",
      "           1       0.65      0.78      0.71       290\n",
      "           2       0.89      0.56      0.68       609\n",
      "           3       0.85      0.74      0.80       465\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1547\n",
      "   macro avg       0.71      0.76      0.70      1547\n",
      "weighted avg       0.78      0.70      0.71      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57      1028\n",
      "           1       0.46      0.88      0.61       183\n",
      "           2       0.22      0.97      0.36        88\n",
      "           3       0.57      0.93      0.70       248\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      1547\n",
      "   macro avg       0.56      0.79      0.56      1547\n",
      "weighted avg       0.82      0.57      0.58      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      1.00      0.04         9\n",
      "           1       0.70      0.77      0.73       314\n",
      "           2       0.92      0.47      0.62       753\n",
      "           3       0.86      0.74      0.79       471\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1547\n",
      "   macro avg       0.62      0.74      0.55      1547\n",
      "weighted avg       0.85      0.62      0.69      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.47      0.64       874\n",
      "           1       0.64      0.90      0.75       246\n",
      "           2       0.26      0.99      0.41       100\n",
      "           3       0.75      0.93      0.83       327\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      1547\n",
      "   macro avg       0.66      0.82      0.66      1547\n",
      "weighted avg       0.84      0.67      0.68      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      1.00      0.41       107\n",
      "           1       0.85      0.76      0.80       385\n",
      "           2       0.89      0.64      0.74       539\n",
      "           3       0.95      0.74      0.83       516\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1547\n",
      "   macro avg       0.74      0.79      0.70      1547\n",
      "weighted avg       0.86      0.73      0.76      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.56      0.71       735\n",
      "           1       0.64      0.97      0.77       230\n",
      "           2       0.65      0.94      0.77       267\n",
      "           3       0.76      0.97      0.85       315\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1547\n",
      "   macro avg       0.76      0.86      0.77      1547\n",
      "weighted avg       0.83      0.77      0.76      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.99      0.59       174\n",
      "           1       0.94      0.67      0.79       486\n",
      "           2       0.82      0.84      0.83       374\n",
      "           3       0.96      0.75      0.84       513\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1547\n",
      "   macro avg       0.79      0.82      0.76      1547\n",
      "weighted avg       0.86      0.78      0.79      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.50      0.67       813\n",
      "           1       0.38      0.99      0.55       134\n",
      "           2       0.68      0.88      0.77       298\n",
      "           3       0.73      0.98      0.84       302\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1547\n",
      "   macro avg       0.70      0.84      0.71      1547\n",
      "weighted avg       0.83      0.71      0.71      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      1.00      0.36        91\n",
      "           1       0.98      0.50      0.66       683\n",
      "           2       0.74      0.88      0.81       321\n",
      "           3       0.91      0.82      0.86       452\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1547\n",
      "   macro avg       0.71      0.80      0.67      1547\n",
      "weighted avg       0.86      0.70      0.73      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.58      0.73       697\n",
      "           1       0.17      1.00      0.29        59\n",
      "           2       0.84      0.79      0.81       409\n",
      "           3       0.90      0.95      0.92       382\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1547\n",
      "   macro avg       0.72      0.83      0.69      1547\n",
      "weighted avg       0.90      0.74      0.78      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.96      0.81       300\n",
      "           1       0.99      0.58      0.73       590\n",
      "           2       0.73      0.94      0.82       298\n",
      "           3       0.87      0.98      0.92       359\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1547\n",
      "   macro avg       0.82      0.87      0.82      1547\n",
      "weighted avg       0.85      0.82      0.81      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86       519\n",
      "           1       0.34      1.00      0.50       117\n",
      "           2       0.89      0.77      0.83       444\n",
      "           3       0.97      0.84      0.90       467\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      1547\n",
      "   macro avg       0.79      0.84      0.77      1547\n",
      "weighted avg       0.90      0.81      0.83      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89       398\n",
      "           1       0.99      0.63      0.77       547\n",
      "           2       0.69      0.96      0.80       274\n",
      "           3       0.80      0.99      0.88       328\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1547\n",
      "   macro avg       0.84      0.87      0.84      1547\n",
      "weighted avg       0.87      0.84      0.83      1547\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90       432\n",
      "           1       0.38      1.00      0.55       133\n",
      "           2       0.93      0.74      0.82       488\n",
      "           3       0.98      0.80      0.88       494\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1547\n",
      "   macro avg       0.81      0.85      0.79      1547\n",
      "weighted avg       0.90      0.82      0.84      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89       472\n",
      "           1       0.99      0.64      0.78       536\n",
      "           2       0.59      0.98      0.73       229\n",
      "           3       0.76      0.99      0.86       310\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1547\n",
      "   macro avg       0.82      0.86      0.81      1547\n",
      "weighted avg       0.87      0.82      0.82      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.87       346\n",
      "           1       0.48      1.00      0.65       168\n",
      "           2       0.97      0.66      0.79       562\n",
      "           3       0.98      0.84      0.90       471\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1547\n",
      "   macro avg       0.81      0.86      0.80      1547\n",
      "weighted avg       0.88      0.82      0.83      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.70      0.82       579\n",
      "           1       0.96      0.71      0.82       469\n",
      "           2       0.47      0.98      0.63       182\n",
      "           3       0.78      0.99      0.87       317\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1547\n",
      "   macro avg       0.80      0.85      0.79      1547\n",
      "weighted avg       0.88      0.80      0.81      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82       287\n",
      "           1       0.69      1.00      0.81       239\n",
      "           2       0.98      0.66      0.79       576\n",
      "           3       0.97      0.88      0.92       445\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1547\n",
      "   macro avg       0.83      0.88      0.84      1547\n",
      "weighted avg       0.88      0.84      0.84      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.80       619\n",
      "           1       0.93      0.82      0.87       391\n",
      "           2       0.48      0.99      0.64       185\n",
      "           3       0.86      0.99      0.92       352\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1547\n",
      "   macro avg       0.81      0.87      0.81      1547\n",
      "weighted avg       0.89      0.82      0.82      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80       275\n",
      "           1       0.86      0.96      0.91       310\n",
      "           2       0.98      0.70      0.82       533\n",
      "           3       0.97      0.91      0.94       429\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1547\n",
      "   macro avg       0.87      0.89      0.87      1547\n",
      "weighted avg       0.90      0.87      0.87      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.70      0.82       584\n",
      "           1       0.90      0.93      0.91       337\n",
      "           2       0.63      0.99      0.77       245\n",
      "           3       0.92      0.98      0.95       381\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1547\n",
      "   macro avg       0.86      0.90      0.86      1547\n",
      "weighted avg       0.90      0.86      0.86      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85       304\n",
      "           1       0.94      0.91      0.92       361\n",
      "           2       0.97      0.78      0.87       476\n",
      "           3       0.97      0.96      0.96       406\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1547\n",
      "   macro avg       0.90      0.91      0.90      1547\n",
      "weighted avg       0.92      0.90      0.90      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.84       560\n",
      "           1       0.86      0.97      0.91       307\n",
      "           2       0.73      0.98      0.84       286\n",
      "           3       0.95      0.97      0.96       394\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1547\n",
      "   macro avg       0.88      0.91      0.89      1547\n",
      "weighted avg       0.91      0.89      0.89      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88       323\n",
      "           1       0.96      0.89      0.92       377\n",
      "           2       0.96      0.83      0.89       448\n",
      "           3       0.97      0.98      0.97       399\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1547\n",
      "   macro avg       0.92      0.92      0.92      1547\n",
      "weighted avg       0.93      0.92      0.92      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.77      0.87       532\n",
      "           1       0.84      0.97      0.90       301\n",
      "           2       0.80      0.97      0.88       316\n",
      "           3       0.96      0.98      0.97       398\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1547\n",
      "   macro avg       0.90      0.92      0.90      1547\n",
      "weighted avg       0.92      0.90      0.90      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92       350\n",
      "           1       0.97      0.88      0.92       381\n",
      "           2       0.96      0.88      0.92       417\n",
      "           3       0.97      0.98      0.98       399\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1547\n",
      "   macro avg       0.94      0.94      0.93      1547\n",
      "weighted avg       0.94      0.93      0.93      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90       502\n",
      "           1       0.86      0.98      0.91       305\n",
      "           2       0.85      0.97      0.90       337\n",
      "           3       0.97      0.97      0.97       403\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1547\n",
      "   macro avg       0.92      0.93      0.92      1547\n",
      "weighted avg       0.93      0.92      0.92      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       379\n",
      "           1       0.97      0.91      0.94       371\n",
      "           2       0.95      0.92      0.94       398\n",
      "           3       0.98      0.99      0.98       399\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1547\n",
      "   macro avg       0.95      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       460\n",
      "           1       0.90      0.97      0.94       322\n",
      "           2       0.90      0.96      0.93       362\n",
      "           3       0.97      0.98      0.98       403\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1547\n",
      "   macro avg       0.94      0.95      0.95      1547\n",
      "weighted avg       0.95      0.95      0.95      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       393\n",
      "           1       0.97      0.93      0.95       361\n",
      "           2       0.96      0.94      0.95       393\n",
      "           3       0.98      0.99      0.99       400\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1547\n",
      "   macro avg       0.96      0.96      0.96      1547\n",
      "weighted avg       0.96      0.96      0.96      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96       440\n",
      "           1       0.93      0.97      0.95       335\n",
      "           2       0.92      0.96      0.94       369\n",
      "           3       0.98      0.99      0.98       403\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1547\n",
      "   macro avg       0.96      0.96      0.96      1547\n",
      "weighted avg       0.96      0.96      0.96      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       408\n",
      "           1       0.97      0.96      0.96       351\n",
      "           2       0.96      0.95      0.95       389\n",
      "           3       0.98      0.99      0.98       399\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.97       428\n",
      "           1       0.96      0.97      0.96       344\n",
      "           2       0.94      0.97      0.95       373\n",
      "           3       0.98      0.99      0.99       402\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       411\n",
      "           1       0.97      0.96      0.97       350\n",
      "           2       0.96      0.96      0.96       385\n",
      "           3       0.98      0.99      0.99       401\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       420\n",
      "           1       0.97      0.97      0.97       349\n",
      "           2       0.95      0.97      0.96       375\n",
      "           3       0.99      0.99      0.99       403\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.97      0.97      0.97      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       414\n",
      "           1       0.97      0.96      0.97       350\n",
      "           2       0.96      0.96      0.96       383\n",
      "           3       0.98      0.99      0.99       400\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.97      0.97      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       416\n",
      "           1       0.97      0.97      0.97       349\n",
      "           2       0.95      0.97      0.96       379\n",
      "           3       0.99      0.99      0.99       403\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.97      0.97      0.97      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       414\n",
      "           1       0.97      0.97      0.97       349\n",
      "           2       0.96      0.97      0.96       381\n",
      "           3       0.99      0.99      0.99       403\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       414\n",
      "           1       0.97      0.97      0.97       348\n",
      "           2       0.96      0.97      0.96       382\n",
      "           3       0.99      0.99      0.99       403\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       414\n",
      "           1       0.97      0.97      0.97       349\n",
      "           2       0.96      0.97      0.96       381\n",
      "           3       0.99      0.99      0.99       403\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       414\n",
      "           1       0.97      0.97      0.97       348\n",
      "           2       0.96      0.97      0.96       381\n",
      "           3       0.99      0.99      0.99       404\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       413\n",
      "           1       0.97      0.97      0.97       348\n",
      "           2       0.96      0.97      0.96       381\n",
      "           3       0.99      0.99      0.99       405\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       414\n",
      "           1       0.97      0.97      0.97       348\n",
      "           2       0.96      0.97      0.96       381\n",
      "           3       0.99      0.99      0.99       404\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       412\n",
      "           1       0.97      0.97      0.97       348\n",
      "           2       0.96      0.97      0.96       381\n",
      "           3       1.00      0.99      0.99       406\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1547\n",
      "   macro avg       0.98      0.98      0.98      1547\n",
      "weighted avg       0.98      0.98      0.98      1547\n",
      "\n",
      "saved synapses to: weights.json\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "# For making a precision, recall report and confusion matrix on the classes\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "anger_training_set = []\n",
    "fear_training_set = []\n",
    "sadness_training_set = []\n",
    "joy_training_set = []\n",
    "stemmer = LancasterStemmer()\n",
    "all_words=[]\n",
    "\n",
    "# Here I am loading the dataset from stored folder. The training data is stored as text file and each tweet is accompanied\n",
    "# by the magnitude of its sentiment (0 to 1). I had to go through the tweets myself and observed that a threshold of 0.5 is \n",
    "# good enough to classify a tweet according to its sentiment. Tweets with lesser threshold were not definitive to be trained as per their mentioned classification  \n",
    "# I only read those tweets that have a dominant classification factor i.e. above 0.5\n",
    "# Here i am setting each tweet's threshold magnitude accordingly\n",
    "def load_training_data(sentiment):\n",
    "    data = open(\"C:/Users/A8/Desktop/ML Project/datasets/\"+sentiment+\"_training_set.txt\",encoding=\"utf8\")\n",
    "    if sentiment == \"anger\":        \n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"fear\":\n",
    "        threshold = 0.6\n",
    "    elif sentiment == \"sadness\":\n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"joy\":\n",
    "        threshold = 0.5\n",
    "    else:\n",
    "        pass\n",
    "    return data,threshold\n",
    "\n",
    "# In this method, I am cleaning the tweet data removing punctuations and then tokenizing the words in tweet removing name tags\n",
    "# and appending them to training set\n",
    "def clean_data(training_data,threshold):\n",
    "    training_set = []\n",
    "    for line in training_data:\n",
    "        line = line.strip().lower()\n",
    "        intensity = float(line.split()[-1])\n",
    "        if (intensity>=threshold):\n",
    "            line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "            punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "            result = line.translate(punct)\n",
    "            tokened_sentence = nltk.word_tokenize(result)\n",
    "            sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "            label = tokened_sentence[-1]\n",
    "            training_set.append((sentence,label))\n",
    "    return training_set\n",
    "    \n",
    "# This method collects all the unique words that are contained in the entire tweet dataset     \n",
    "def bag_of_words(all_data):\n",
    "    training_set = []\n",
    "    all_words = []\n",
    "    for each_list in all_data:\n",
    "        for words in each_list[0]:\n",
    "            word = stemmer.stem(words)\n",
    "            all_words.append(word)\n",
    "    all_words = list(set(all_words))\n",
    "    \n",
    "    for each_sentence in all_data:  \n",
    "        bag = [0]*len(all_words)\n",
    "        training_set.append(encode_sentence(all_words,each_sentence[0],bag))\n",
    "    return training_set,all_words\n",
    "\n",
    "# Here we encode each tweet's words according to the words it contained from the bag of words which is based on all words in all tweets\n",
    "def encode_sentence(all_words,sentence, bag):\n",
    "    for s in sentence:        \n",
    "        stemmed_word = stemmer.stem(s)\n",
    "        for i,word in enumerate(all_words):\n",
    "            if stemmed_word == word:\n",
    "                bag[i] = 1\n",
    "    return bag\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    bag = [] \n",
    "    all_data = []\n",
    "    labels = []\n",
    "    classes = []\n",
    "    \n",
    "    ######### Here we read the whole training data for each class and the threshold we will use for its classification\n",
    "    anger_training_data,threshold = load_training_data(\"anger\")\n",
    "    anger_training_set = clean_data(anger_training_data,threshold)\n",
    "    \n",
    "    fear_training_data,threshold = load_training_data(\"fear\")\n",
    "    fear_training_set = clean_data(fear_training_data,threshold)\n",
    "    \n",
    "    sadness_training_data,threshold = load_training_data(\"sadness\")\n",
    "    sadness_training_set = clean_data(sadness_training_data,threshold)\n",
    "    \n",
    "    joy_training_data,threshold = load_training_data(\"joy\")\n",
    "    joy_training_set = clean_data(joy_training_data,threshold)\n",
    "    \n",
    "    ###### In every training set above we have a nested list whose first element is sentence and 2nd element its respective label ######\n",
    "    \n",
    "#    print(anger_training_set[0][0],anger_training_set[0][1])\n",
    "#    print(joy_training_set[0][0],joy_training_set[0][1])\n",
    "    \n",
    "    ###### Here we combine all training sets in one list ######\n",
    "    all_data.extend(anger_training_set)\n",
    "    all_data.extend(fear_training_set)\n",
    "    all_data.extend(sadness_training_set)\n",
    "    all_data.extend(joy_training_set)\n",
    "    \n",
    "    ###### Here we simply make a classification label list encoding our 4 classes as follows\n",
    "    labels = []\n",
    "    for i,j in all_data:\n",
    "        if j == \"anger\":            \n",
    "            labels.append([1,0,0,0])\n",
    "        elif j == \"fear\":            \n",
    "            labels.append([0,1,0,0])\n",
    "        elif j == \"sadness\":            \n",
    "            labels.append([0,0,1,0])\n",
    "        elif j == \"joy\":            \n",
    "            labels.append([0,0,0,1])\n",
    "        else:\n",
    "            pass\n",
    "    classes = [\"anger\",\"fear\",\"sadness\",\"joy\"]\n",
    "    print(classes)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    words=[]\n",
    "    \n",
    "    # Here we will have the whole training set and the all the words contained in whole training set\n",
    "    training_set,words = bag_of_words(all_data)\n",
    "    \n",
    "    # We convert our training set in a numpy array as it is required for calculations in neural net\n",
    "    training_set = np.array(training_set)\n",
    "    \n",
    "    # We convert our labels in numpy array as it is required for calculations in neural net\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # It is important to shuffle dataset so your classifier does not attempt to memorize training set, this functions shuffles data and labels.\n",
    "    shuffling_function = np.random.permutation(training_set.shape[0])\n",
    "    shuffled_training_set, shuffled_labels = np.zeros((training_set.shape)),np.zeros((training_set.shape))\n",
    "    shuffled_training_set,shuffled_labels= training_set[shuffling_function],labels[shuffling_function]\n",
    "    \n",
    "    ############# HERE WE HAVE A SHUFFLED DATASET WITH RESPECTIVE LABELS NOW WE HAVE TO TRAIN THIS DATA BOTH NUMPY ARRAYS ############\n",
    "    Train_model(shuffled_training_set,shuffled_labels,words,classes)\n",
    "\n",
    "# Method for calculating sigmoid\n",
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))\n",
    "    \n",
    "# Method for calculating relu\n",
    "def relu(z):\n",
    "    A = np.array(z,copy=True)\n",
    "    A[z<0]=0\n",
    "    assert A.shape == z.shape\n",
    "    return A\n",
    "    \n",
    "# Method for calculating softmax\n",
    "def softmax(x):\n",
    "    num = np.exp(x-np.amax(x,axis=0,keepdims=True))    \n",
    "    return num/np.sum(num,axis=0,keepdims=True)\n",
    "\n",
    "# Method for calculating forward propagation\n",
    "def forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2):\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "\n",
    "# Method for calculating relu activation's derivative\n",
    "def relu_backward(da,dz):\n",
    "    da1 = np.array(da,copy=True)\n",
    "    da1[dz<0]=0\n",
    "    assert da1.shape == dz.shape\n",
    "    return da1\n",
    "\n",
    "# Method for calculating linear part of backward propagation\n",
    "def linear_backward(dz,a,m,w,b):\n",
    "    dw = (1/m)*np.dot(dz,a.T)\n",
    "    db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
    "    da = np.dot(w.T,dz)\n",
    "    assert (dw.shape==w.shape)\n",
    "    assert (da.shape==a.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return da,dw,db \n",
    "\n",
    "# Method for calculating loss function\n",
    "def calculate_loss(Y,Yhat,m):\n",
    "    loss = (-1/m)*np.sum(np.multiply(Y,np.log(Yhat)))\n",
    "    return loss\n",
    "\n",
    "# Method for back propagation\n",
    "def back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m):\n",
    "    dZ2 = A2-Y\n",
    "    da1,dw2,db2 = linear_backward(dZ2,A1,m,W2,b2)\n",
    "    dZ1 = relu_backward(da1,Z1)\n",
    "    da0,dw1,db1 = linear_backward(dZ1,X,m,W1,b1)\n",
    "    W2 = W2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "# Method for training model\n",
    "def Train_model(training_data, training_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = training_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = training_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    # Multiplying by 0.01 so that we get smaller weights .. dimensions 100x3787\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    print(\" Shape of W1 is \", W1.shape)\n",
    "    # Dimensions 100x1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    # Dimensions 1547 x 4\n",
    "    W2 = np.random.randn(n_y,n_h)\n",
    "    print(\" Shape of W2 is \", W2.shape)\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    for i in range(0,iterations):\n",
    "        Z1,A1,Z2,A2 = forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2)\n",
    "        predictions = np.argmax(A2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "        Loss = calculate_loss(Y,A2,m)\n",
    "        W1,b1,W2,b2 = back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m)\n",
    "        all_losses.append(Loss)\n",
    "\n",
    "    # storing weights so that we can reuse them without having to retrain the neural network\n",
    "    weights = {'weight1': W1.tolist(), 'weight2': W2.tolist(), \n",
    "               'bias1':b1.tolist(), 'bias2':b2.tolist(),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    weights_file = \"weights.json\"\n",
    "\n",
    "    with open(weights_file, 'w') as outfile:\n",
    "        json.dump(weights, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", weights_file)\n",
    "    plt.plot(all_losses)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is  (3787, 1)\n",
      "I want to kill everyone @Shayan #why? \n",
      " classification: [['anger', array([0.76028069])]]\n",
      "Shape of X is  (3787, 1)\n",
      "I am so happy @Jaron #yayyyy \n",
      " classification: [['joy', array([0.79523645])]]\n",
      "Shape of X is  (3787, 1)\n",
      "This depression will kill me someday .. i am dying @Manaswi #killme \n",
      " classification: [['sadness', array([0.65506426])]]\n",
      "Shape of X is  (3787, 1)\n",
      "I am afraid terrorists might attack us @jackson #isis \n",
      " classification: [['fear', array([0.99103431])]]\n",
      "Shape of X is  (3787, 1)\n",
      "What should I do when i am happy @John  \n",
      " classification: [['anger', array([0.36538307])], ['joy', array([0.27742402])]]\n",
      "Shape of X is  (3787, 1)\n",
      "I want to be happy \n",
      " classification: [['joy', array([0.76523816])]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['joy', array([0.76523816])]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.2\n",
    "# load our calculated weight values\n",
    "weights_file = 'weights.json' \n",
    "with open(weights_file) as data_file: \n",
    "    weights = json.load(data_file) \n",
    "    W1 = np.asarray(weights['weight1']) \n",
    "    W2 = np.asarray(weights['weight2'])\n",
    "    b1 = np.asarray(weights['bias1']) \n",
    "    b2 = np.asarray(weights['bias2'])\n",
    "    all_words = weights['words']\n",
    "    classes = weights['classes']\n",
    "    \n",
    "def clean_sentence(verification_data):\n",
    "    line = verification_data\n",
    "    # Remove whitespace from line and lower case iter\n",
    "    line = line.strip().lower()\n",
    "    # Removing word with @ sign as we dont need name tags of twitter\n",
    "    line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "    # Remove punctuations and numbers from the line\n",
    "    punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "    result = line.translate(punct)\n",
    "    # Tokenize the whole tweet sentence\n",
    "    tokened_sentence = nltk.word_tokenize(result)\n",
    "    # We take the tweet sentence from tokened sentence\n",
    "    sentence = tokened_sentence[0:len(tokened_sentence)]\n",
    "    return sentence    \n",
    "\n",
    "def verify(sentence, show_details=False):\n",
    "    bag=[0]*len(all_words)\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    # This line returns the bag of words as 0 or 1 if words in sentence are found in all_words\n",
    "    x = encode_sentence(all_words,cleaned_sentence,bag)\n",
    "    x = np.array(x)\n",
    "    x = x.reshape(x.shape[0],1)\n",
    "    \n",
    "    print(\"Shape of X is \", x.shape)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our encoded sentence\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = relu(np.dot(W1,l0)+b1)\n",
    "    # output layer\n",
    "    l2 = softmax(np.dot(W2,l1)+b2)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = verify(sentence, show_details)\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    print (\"%s \\n classification: %s\" % (sentence, return_results))\n",
    "    return return_results\n",
    "\n",
    "classify(\"I want to kill everyone @Name1 #why?\")\n",
    "classify(\"I am so happy @Name2 #yayyyy\")\n",
    "classify(\"This depression will kill me someday .. i am dying @Name3 #killme\")\n",
    "classify(\"I am afraid terrorists might attack us @Name4 #isis\")\n",
    "classify(\"What should I do when i am happy @Name5 \")\n",
    "classify(\"I want to be happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
